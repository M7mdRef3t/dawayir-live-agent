import React, { useCallback, useEffect, useMemo, useRef, useState } from 'react';
import DawayirCanvas from './components/DawayirCanvas';
import './App.css';

const INPUT_SAMPLE_RATE = 16000;
const OUTPUT_SAMPLE_RATE = 24000;
const MIC_WORKLET_NAME = 'dawayir-mic-processor';
const MAX_RECONNECT_ATTEMPTS = 2;
const RECONNECT_DELAY_MS = 1200;

const arrayBufferToBase64 = (arrayBuffer) => {
  const bytes = new Uint8Array(arrayBuffer);
  let binary = '';
  const chunkSize = 0x8000;

  for (let i = 0; i < bytes.length; i += chunkSize) {
    binary += String.fromCharCode(...bytes.subarray(i, i + chunkSize));
  }

  return window.btoa(binary);
};

const base64ToArrayBuffer = (base64) => {
  const binary = window.atob(base64);
  const bytes = new Uint8Array(binary.length);

  for (let i = 0; i < binary.length; i += 1) {
    bytes[i] = binary.charCodeAt(i);
  }

  return bytes.buffer;
};

const pcm16ToFloat32 = (arrayBuffer) => {
  const dataView = new DataView(arrayBuffer);
  const float32 = new Float32Array(arrayBuffer.byteLength / 2);

  for (let i = 0; i < float32.length; i += 1) {
    const int16 = dataView.getInt16(i * 2, true);
    float32[i] = int16 / 32768;
  }

  return float32;
};

const float32ToPcm16Buffer = (float32Samples) => {
  const int16 = new Int16Array(float32Samples.length);
  for (let i = 0; i < float32Samples.length; i += 1) {
    const sample = Math.max(-1, Math.min(1, float32Samples[i]));
    int16[i] = sample < 0 ? sample * 32768 : sample * 32767;
  }
  return int16.buffer;
};

const tryParseJson = (text) => {
  try {
    return JSON.parse(text);
  } catch {
    return null;
  }
};

const getInlineData = (part) => part?.inlineData ?? part?.inline_data;
const getServerContent = (message) => message?.serverContent ?? message?.server_content;
const getModelTurn = (message) =>
  getServerContent(message)?.modelTurn ?? getServerContent(message)?.model_turn;
const getParts = (message) => getModelTurn(message)?.parts ?? [];
const getToolCall = (message) => message?.toolCall ?? message?.tool_call;
const isSetupCompleteMessage = (message) =>
  Boolean(message?.setupComplete || message?.setup_complete);
const getServerErrorMessage = (message) =>
  message?.serverError?.message ?? message?.server_error?.message;
const isInterruptedMessage = (message) =>
  Boolean(getServerContent(message)?.interrupted ?? getServerContent(message)?.is_interrupted);
const isAudioMimeType = (mimeType = '') => mimeType.startsWith('audio/pcm');
const normalizeBlob = (blob) => {
  const data = blob?.data;
  const mimeType = blob?.mimeType ?? blob?.mime_type;
  if (typeof data !== 'string') return null;
  return {
    data,
    mimeType: typeof mimeType === 'string' ? mimeType : 'audio/pcm',
  };
};
const getTurnDataAudioBlobs = (message) => {
  const turnData = message?.turn?.data;
  if (!turnData) return [];

  const candidates = Array.isArray(turnData) ? turnData : [turnData];
  return candidates
    .map((candidate) => (typeof candidate === 'string'
      ? { data: candidate, mimeType: 'audio/pcm' }
      : normalizeBlob(candidate)))
    .filter((blob) => blob && isAudioMimeType(blob.mimeType));
};

const Visualizer = ({ stream, isConnected }) => {
  const canvasRef = useRef(null);

  useEffect(() => {
    if (!stream || !isConnected) return;

    const canvas = canvasRef.current;
    if (!canvas) return;
    const ctx = canvas.getContext('2d');
    const audioContext = new (window.AudioContext || window.webkitAudioContext)();
    const source = audioContext.createMediaStreamSource(stream);
    const analyser = audioContext.createAnalyser();

    analyser.fftSize = 256;
    source.connect(analyser);

    const bufferLength = analyser.frequencyBinCount;
    const dataArray = new Uint8Array(bufferLength);

    let animationFrameId;

    const draw = () => {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      analyser.getByteFrequencyData(dataArray);

      const barWidth = (canvas.width / bufferLength) * 2.5;
      let barHeight;
      let x = 0;

      for (let i = 0; i < bufferLength; i++) {
        barHeight = (dataArray[i] / 255) * canvas.height;
        ctx.fillStyle = `rgba(0, 245, 255, ${dataArray[i] / 255})`;
        ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);
        x += barWidth + 1;
      }

      animationFrameId = requestAnimationFrame(draw);
    };

    draw();

    return () => {
      cancelAnimationFrame(animationFrameId);
      analyser.disconnect();
      source.disconnect();
      audioContext.close();
    };
  }, [stream, isConnected]);

  return <canvas ref={canvasRef} className="visualizer" width="300" height="60" />;
};

function App() {
  const [status, setStatus] = useState('Disconnected');
  const [errorMessage, setErrorMessage] = useState('');
  const [isConnected, setIsConnected] = useState(false);
  const [isStarting, setIsStarting] = useState(false);
  const [isMicActive, setIsMicActive] = useState(false);
  const [toolCallsCount, setToolCallsCount] = useState(0);
  const [lastEvent, setLastEvent] = useState('none');
  const [reconnectAttempt, setReconnectAttempt] = useState(0);

  const wsRef = useRef(null);
  const canvasRef = useRef(null);
  const setupCompleteRef = useRef(false);
  const manualCloseRef = useRef(false);
  const reconnectTimeoutRef = useRef(null);
  const reconnectAttemptRef = useRef(0);

  const micStreamRef = useRef(null);
  const micContextRef = useRef(null);
  const micSourceRef = useRef(null);
  const micWorkletRef = useRef(null);
  const micProcessorRef = useRef(null);
  const micSilentGainRef = useRef(null);
  const bootstrapPromptSentRef = useRef(false);

  const speakerContextRef = useRef(null);
  const activeSourcesRef = useRef(new Set());
  const nextPlaybackTimeRef = useRef(0);

  const backendUrl = useMemo(() => {
    const envUrl = import.meta.env.VITE_BACKEND_WS_URL;
    if (envUrl) return envUrl;

    if (typeof window === 'undefined') return 'ws://localhost:8080';

    const isLocalHost = window.location.hostname === 'localhost' || window.location.hostname === '127.0.0.1';
    if (isLocalHost) return 'ws://localhost:8080';

    const wsProtocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
    return `${wsProtocol}//${window.location.host}`;
  }, []);

  const statusClass = useMemo(() => {
    const normalized = status.toLowerCase();

    if (normalized.includes('connected')) return 'connected';
    if (normalized.includes('connecting')) return 'connecting';
    if (normalized.includes('error')) return 'error';
    return 'disconnected';
  }, [status]);

  const getAudioContextCtor = () => window.AudioContext || window.webkitAudioContext;

  const closeSpeakerContext = useCallback(async () => {
    if (!speakerContextRef.current) return;

    try {
      await speakerContextRef.current.close();
    } catch {
      // Ignore close race errors.
    }

    speakerContextRef.current = null;
    nextPlaybackTimeRef.current = 0;
    activeSourcesRef.current.clear();
  }, []);

  const stopPlayback = useCallback(() => {
    for (const source of activeSourcesRef.current) {
      try {
        source.stop();
      } catch {
        // Ignore if source already stopped.
      }

      try {
        source.disconnect();
      } catch {
        // Ignore disconnect errors.
      }
    }

    activeSourcesRef.current.clear();

    if (speakerContextRef.current) {
      nextPlaybackTimeRef.current = speakerContextRef.current.currentTime;
    } else {
      nextPlaybackTimeRef.current = 0;
    }
  }, []);

  const ensureSpeakerContext = useCallback(async () => {
    if (speakerContextRef.current) {
      if (speakerContextRef.current.state === 'suspended') {
        await speakerContextRef.current.resume();
      }
      return speakerContextRef.current;
    }

    const AudioContextCtor = getAudioContextCtor();
    if (!AudioContextCtor) {
      throw new Error('Web Audio API is not supported in this browser.');
    }

    const ctx = new AudioContextCtor({ sampleRate: OUTPUT_SAMPLE_RATE });
    if (ctx.state === 'suspended') {
      await ctx.resume();
    }

    speakerContextRef.current = ctx;
    nextPlaybackTimeRef.current = ctx.currentTime;
    return ctx;
  }, []);

  const playPcmChunk = useCallback(
    async (arrayBuffer) => {
      if (!arrayBuffer || arrayBuffer.byteLength === 0) return;

      const audioContext = await ensureSpeakerContext();
      const float32 = pcm16ToFloat32(arrayBuffer);
      if (float32.length === 0) return;

      const audioBuffer = audioContext.createBuffer(1, float32.length, OUTPUT_SAMPLE_RATE);
      audioBuffer.getChannelData(0).set(float32);

      const source = audioContext.createBufferSource();
      source.buffer = audioBuffer;
      source.connect(audioContext.destination);

      const startAt = Math.max(audioContext.currentTime + 0.02, nextPlaybackTimeRef.current);
      source.start(startAt);
      nextPlaybackTimeRef.current = startAt + audioBuffer.duration;

      activeSourcesRef.current.add(source);
      source.onended = () => {
        activeSourcesRef.current.delete(source);
        try {
          source.disconnect();
        } catch {
          // Ignore disconnect errors.
        }
      };
    },
    [ensureSpeakerContext]
  );

  const stopMicrophone = useCallback(async () => {
    if (micSourceRef.current) {
      try {
        micSourceRef.current.disconnect();
      } catch {
        // Ignore disconnect errors.
      }
      micSourceRef.current = null;
    }

    if (micWorkletRef.current) {
      try {
        micWorkletRef.current.disconnect();
      } catch {
        // Ignore disconnect errors.
      }
      micWorkletRef.current = null;
    }

    if (micProcessorRef.current) {
      try {
        micProcessorRef.current.onaudioprocess = null;
        micProcessorRef.current.disconnect();
      } catch {
        // Ignore disconnect errors.
      }
      micProcessorRef.current = null;
    }

    if (micSilentGainRef.current) {
      try {
        micSilentGainRef.current.disconnect();
      } catch {
        // Ignore disconnect errors.
      }
      micSilentGainRef.current = null;
    }

    if (micStreamRef.current) {
      micStreamRef.current.getTracks().forEach((track) => track.stop());
      micStreamRef.current = null;
    }

    if (micContextRef.current) {
      try {
        await micContextRef.current.close();
      } catch {
        // Ignore close race errors.
      }
      micContextRef.current = null;
    }

    setIsMicActive(false);
  }, []);

  const sendRealtimeAudioChunk = useCallback((arrayBuffer, sampleRate) => {
    if (!arrayBuffer || !setupCompleteRef.current) return;

    const socket = wsRef.current;
    if (!socket || socket.readyState !== WebSocket.OPEN) return;

    socket.send(
      JSON.stringify({
        realtimeInput: {
          mediaChunks: [
            {
              mimeType: `audio/pcm;rate=${sampleRate}`,
              data: arrayBufferToBase64(arrayBuffer),
            },
          ],
        },
      })
    );
  }, []);

  const startMicrophone = useCallback(async () => {
    await stopMicrophone();

    if (!navigator.mediaDevices?.getUserMedia) {
      throw new Error('Microphone capture is not supported in this browser.');
    }

    const AudioContextCtor = getAudioContextCtor();
    if (!AudioContextCtor) {
      throw new Error('Web Audio API is not supported in this browser.');
    }

    const stream = await navigator.mediaDevices.getUserMedia({
      audio: {
        channelCount: 1,
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      },
    });

    const micContext = new AudioContextCtor({ sampleRate: INPUT_SAMPLE_RATE });
    if (micContext.state === 'suspended') {
      await micContext.resume();
    }

    const source = micContext.createMediaStreamSource(stream);
    const silentGain = micContext.createGain();
    silentGain.gain.value = 0;
    let workletReady = false;

    if (micContext.audioWorklet && typeof AudioWorkletNode !== 'undefined') {
      try {
        await micContext.audioWorklet.addModule(new URL('./audio/mic-processor.js', import.meta.url));
        const worklet = new AudioWorkletNode(micContext, MIC_WORKLET_NAME);
        worklet.port.onmessage = (event) => {
          const int16arrayBuffer = event.data?.int16arrayBuffer;
          if (!int16arrayBuffer) return;
          sendRealtimeAudioChunk(int16arrayBuffer, micContext.sampleRate);
        };

        source.connect(worklet);
        worklet.connect(silentGain);
        micWorkletRef.current = worklet;
        workletReady = true;
      } catch {
        // Fall back to ScriptProcessor when Worklet fails on specific browsers/extensions.
      }
    }

    if (!workletReady) {
      const processor = micContext.createScriptProcessor(2048, 1, 1);
      processor.onaudioprocess = (event) => {
        const input = event.inputBuffer?.getChannelData(0);
        if (!input || input.length === 0) return;
        sendRealtimeAudioChunk(float32ToPcm16Buffer(input), micContext.sampleRate);
      };

      source.connect(processor);
      processor.connect(silentGain);
      micProcessorRef.current = processor;
    }

    // Keep microphone graph active without audible loopback.
    silentGain.connect(micContext.destination);

    micStreamRef.current = stream;
    micContextRef.current = micContext;
    micSourceRef.current = source;
    micSilentGainRef.current = silentGain;
    setIsMicActive(true);
  }, [sendRealtimeAudioChunk, stopMicrophone]);

  const handleToolCall = useCallback((toolCall) => {
    const functionCalls = Array.isArray(toolCall?.functionCalls)
      ? toolCall.functionCalls
      : Array.isArray(toolCall?.function_calls)
        ? toolCall.function_calls
        : [];
    if (functionCalls.length === 0) return;

    const responses = [];

    for (const call of functionCalls) {
      let args = call?.args ?? {};
      if (typeof args === 'string') {
        try {
          args = JSON.parse(args);
        } catch {
          args = {};
        }
      }

      try {
        if (call.name === 'update_node') {
          const id = Number(args.id);
          if (!Number.isFinite(id)) {
            throw new Error('Invalid node id for update_node.');
          }

          const updates = { ...args };
          delete updates.id;
          canvasRef.current?.updateNode(id, updates);
        } else if (call.name === 'highlight_node') {
          const id = Number(args.id);
          if (!Number.isFinite(id)) {
            throw new Error('Invalid node id for highlight_node.');
          }

          canvasRef.current?.pulseNode(id);
        } else if (call.name === 'save_mental_map') {
          const nodes = canvasRef.current?.getNodes() || [];
          responses.push({
            id: call.id,
            name: call.name,
            response: { result: { ok: true, nodes } },
          });
          continue; // Skip the default push at the end
        } else {
          throw new Error(`Unsupported tool: ${call.name}`);
        }

        responses.push({
          id: call.id,
          name: call.name,
          response: { result: { ok: true } },
        });
      } catch (error) {
        responses.push({
          id: call.id,
          name: call.name,
          response: { result: { ok: false, error: error.message } },
        });
      }
    }

    const socket = wsRef.current;
    if (socket && socket.readyState === WebSocket.OPEN && responses.length > 0) {
      socket.send(
        JSON.stringify({
          toolResponse: {
            functionResponses: responses,
          },
        })
      );
    }
    setToolCallsCount((prev) => prev + functionCalls.length);
    setLastEvent(`tool_call:${functionCalls.length}`);
  }, []);

  const disconnect = useCallback(async () => {
    manualCloseRef.current = true;
    if (reconnectTimeoutRef.current) {
      window.clearTimeout(reconnectTimeoutRef.current);
      reconnectTimeoutRef.current = null;
    }
    reconnectAttemptRef.current = 0;
    setReconnectAttempt(0);
    const socket = wsRef.current;
    wsRef.current = null;

    setupCompleteRef.current = false;
    bootstrapPromptSentRef.current = false;
    setIsStarting(false);
    setIsConnected(false);

    await stopMicrophone();
    stopPlayback();
    await closeSpeakerContext();

    if (socket && socket.readyState === WebSocket.OPEN) {
      try {
        socket.send(JSON.stringify({ realtimeInput: { audioStreamEnd: true } }));
      } catch {
        // Ignore send errors while closing.
      }
      socket.close();
    }

    setStatus('Disconnected');
    setLastEvent('manual_disconnect');
  }, [closeSpeakerContext, stopMicrophone, stopPlayback]);

  const connect = useCallback(async () => {
    if (isStarting || isConnected) return;
    manualCloseRef.current = false;

    setErrorMessage('');
    setIsStarting(true);
    setStatus('Connecting...');
    setLastEvent('connecting');

    try {
      await ensureSpeakerContext();
    } catch (error) {
      setStatus('Error');
      setErrorMessage(error.message);
      setIsStarting(false);
      setLastEvent('speaker_context_error');
      return;
    }

    const socket = new WebSocket(backendUrl);
    socket.binaryType = 'arraybuffer';
    wsRef.current = socket;

    socket.onopen = () => {
      reconnectAttemptRef.current = 0;
      setReconnectAttempt(0);
      setIsConnected(true);
      setIsStarting(false);
      setStatus('Connected (waiting setupComplete)');
      setLastEvent('ws_open');
    };

    socket.onmessage = async (event) => {
      let message = null;

      if (typeof event.data === 'string') {
        message = tryParseJson(event.data);
      }

      if (event.data instanceof ArrayBuffer) {
        const decoded = new TextDecoder('utf-8').decode(event.data);
        message = tryParseJson(decoded);
        if (!message) {
          await playPcmChunk(event.data);
          return;
        }
      }

      if (event.data instanceof Blob) {
        const arrayBuffer = await event.data.arrayBuffer();
        const decoded = new TextDecoder('utf-8').decode(arrayBuffer);
        message = tryParseJson(decoded);
        if (!message) {
          await playPcmChunk(arrayBuffer);
          return;
        }
      }

      if (!message) return;

      const serverError = getServerErrorMessage(message);
      if (serverError) {
        setStatus('Error');
        setErrorMessage(serverError);
        setLastEvent('server_error');
        return;
      }

      if (isSetupCompleteMessage(message)) {
        setupCompleteRef.current = true;
        setStatus('Connected to Gemini Live');
        setLastEvent('setup_complete');

        try {
          await startMicrophone();
          if (!bootstrapPromptSentRef.current && wsRef.current?.readyState === WebSocket.OPEN) {
            bootstrapPromptSentRef.current = true;
            wsRef.current.send(
              JSON.stringify({
                clientContent: {
                  turns: [
                    {
                      role: 'user',
                      parts: [
                        {
                          text: `أهلًا بك! أنا "دوائر"، رفيقك في رحلة استكشاف مساحتك الذهنية. 
                          ابدأ بالترحيب بالمستخدم بلهجة مصرية ودودة جداً وبأسلوب هادئ،
                          واطلب منه أن يعبر عن مشاعره أو أفكاره الحالية بصوته لكي نراها سوياً في الدوائر التي أمامه.`,
                        },
                      ],
                    },
                  ],
                  turnComplete: true,
                },
              })
            );
          }
        } catch (error) {
          setStatus('Error');
          setErrorMessage(error.message);
          setLastEvent('mic_start_error');
        }
        return;
      }

      const toolCall = getToolCall(message);
      if (toolCall) {
        handleToolCall(toolCall);
      }

      if (isInterruptedMessage(message)) {
        stopPlayback();
        setLastEvent('server_interrupted');
      }

      const parts = getParts(message);
      const audioParts = Array.isArray(parts)
        ? parts.filter((part) =>
          isAudioMimeType(getInlineData(part)?.mimeType)
          || isAudioMimeType(getInlineData(part)?.mime_type)
        )
        : [];

      for (const part of audioParts) {
        const inline = getInlineData(part);
        if (inline?.data) {
          await playPcmChunk(base64ToArrayBuffer(inline.data));
          setLastEvent('audio_chunk');
        }
      }

      const turnAudioBlobs = getTurnDataAudioBlobs(message);
      if (audioParts.length === 0 && turnAudioBlobs.length === 0) return;

      for (const blob of turnAudioBlobs) {
        if (blob?.data) {
          await playPcmChunk(base64ToArrayBuffer(blob.data));
          setLastEvent('audio_chunk_turn_data');
        }
      }
    };

    socket.onerror = () => {
      setStatus('Error');
      setErrorMessage('WebSocket error. Retrying if possible.');
      setIsStarting(false);
      setLastEvent('ws_error');
    };

    socket.onclose = async () => {
      wsRef.current = null;
      setupCompleteRef.current = false;
      bootstrapPromptSentRef.current = false;
      setIsConnected(false);
      setIsStarting(false);

      await stopMicrophone();
      stopPlayback();

      if (manualCloseRef.current) {
        setStatus((prev) => (prev === 'Error' ? prev : 'Disconnected'));
        setLastEvent('ws_closed_manual');
        return;
      }

      const nextAttempt = reconnectAttemptRef.current + 1;
      if (nextAttempt <= MAX_RECONNECT_ATTEMPTS) {
        reconnectAttemptRef.current = nextAttempt;
        setReconnectAttempt(nextAttempt);
        setStatus(`Reconnecting (${nextAttempt}/${MAX_RECONNECT_ATTEMPTS})...`);
        setLastEvent('ws_closed_retrying');
        reconnectTimeoutRef.current = window.setTimeout(() => {
          connect();
        }, RECONNECT_DELAY_MS);
        return;
      }

      setStatus((prev) => (prev === 'Error' ? prev : 'Disconnected'));
      setErrorMessage('Connection closed after retry attempts. Please reconnect manually.');
      setLastEvent('ws_closed_giveup');
    };
  }, [
    backendUrl,
    ensureSpeakerContext,
    handleToolCall,
    isConnected,
    isStarting,
    playPcmChunk,
    startMicrophone,
    stopMicrophone,
    stopPlayback,
  ]);

  useEffect(() => {
    return () => {
      disconnect();
    };
  }, [disconnect]);

  return (
    <div className="App">
      <div className="overlay">
        <header>
          <h1>Dawayir Live Agent</h1>
          <div className={`status-badge ${statusClass}`}>
            <span className="dot"></span> {isConnected ? 'Live Session Active' : status}
          </div>
        </header>

        <section className="main-controls">
          <button className="primary-btn" onClick={connect} disabled={isConnected || isStarting}>
            {isConnected
              ? '✨ Connection Secured'
              : isStarting
                ? 'Establishing Link...'
                : 'Start Your Journey'}
          </button>

          {isConnected && (
            <div className="activity-container">
              <Visualizer stream={micStreamRef.current} isConnected={isConnected} />
              <button className="secondary disconnect-btn" onClick={disconnect}>
                Finish Session
              </button>
            </div>
          )}
        </section>

        {isConnected && !errorMessage && (
          <p className="hint">
            Speak naturally and explore your mental space.
          </p>
        )}

        {errorMessage && <p className="error-message">⚠️ {errorMessage}</p>}

        <footer className="footer-info">
          <span>Backend: {backendUrl}</span>
          <br />
          <span>
            Mic: {isMicActive ? 'on' : 'off'} | Retries: {reconnectAttempt}/{MAX_RECONNECT_ATTEMPTS} | Tools: {toolCallsCount} | Event: {lastEvent}
          </span>
        </footer>
      </div>
      <DawayirCanvas ref={canvasRef} />
    </div>
  );
}

export default App;
